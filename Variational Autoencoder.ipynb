{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd4094d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "import random\n",
    "from sklearn.manifold import TSNE\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.manifold import TSNE\n",
    "import cv2 # pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3238df12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://debuggercafe.com/getting-started-with-variational-autoencoder-using-pytorch/\n",
    "class VariationalAutoencoder(nn.Module):\n",
    "    def __init__(self, num_features=8, num_dim=784):\n",
    "        super(VariationalAutoencoder, self).__init__()\n",
    "        \n",
    "        self.num_features = num_features\n",
    "        self.num_dim = num_dim\n",
    "        \n",
    "        self.encoder_layer_1 = nn.Linear(in_features=self.num_dim, out_features=512)\n",
    "        self.encoder_layer_2 = nn.Linear(in_features=512, out_features=(self.num_features * 2))\n",
    "        \n",
    "        self.decoder_layer_1 = nn.Linear(in_features=self.num_features, out_features=512)\n",
    "        self.decoder_layer_2 = nn.Linear(in_features=512, out_features=self.num_dim)\n",
    "        \n",
    "    def reparameterize(self, mu, log_var):\n",
    "        \"\"\"\n",
    "        :param mu: mean from the encoder's latent space\n",
    "        :param log_var: log variance from the encoder's latent space\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5*log_var) # standard deviation\n",
    "        eps = torch.randn_like(std)  # `randn_like` as we need the same size\n",
    "        sample = mu + (eps * std)    # sampling as if coming from the input space\n",
    "        \n",
    "        return sample\n",
    "    \n",
    "    def encode(self, x):\n",
    "        # encoding\n",
    "        x = F.relu(self.encoder_layer_1(x))\n",
    "        x = self.encoder_layer_2(x).view(-1, 2, self.num_features)\n",
    "        \n",
    "        # get `mu` and `log_var`\n",
    "        mu = x[:, 0, :] # the first feature values as mean\n",
    "        log_var = x[:, 1, :] # the other feature values as variance\n",
    "        \n",
    "        # get the latent vector through reparameterization\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        \n",
    "        return z, mu, log_var\n",
    "    \n",
    "    def decode(self, z, mu, log_var):\n",
    "        # decoding\n",
    "        x = F.relu(self.decoder_layer_1(z))\n",
    "        reconstruction = torch.sigmoid(self.decoder_layer_2(x))\n",
    "        \n",
    "        return reconstruction, mu, log_var\n",
    "    \n",
    "    def sample(self, mu, log_var):\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        reconstruction, mu, log_var = self.decode(z, mu, log_var)\n",
    "        \n",
    "        return reconstruction\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        z, mu, log_var = self.encode(x)\n",
    "        reconstruction, mu, log_var = self.decode(z, mu, log_var)\n",
    "        \n",
    "        return reconstruction, mu, log_var"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
